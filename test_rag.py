"""Utility script to evaluate RAG answers with an LLM judge and log results."""

import json
import time
from typing import Tuple, List, Dict, Any

from rag_core import generate_answer
from config import client, JUDGE_MODEL

ATTEMPTS_PER_QUESTION = 3
LOG_FILE = "rag_test_results_2.jsonl"  # JSONL safe append
QUESTIONS_FILE = "tts_questions.json"


def load_questions(path: str) -> List[Dict[str, Any]]:
    """Load questions from a JSON structure.

    Expected JSON format:
        {
            "section_name": ["question 1", "question 2", ...],
            ...
        }

    Returns:
        A flat list of dicts with keys: "section", "question".
    """
    with open(path, "r", encoding="utf-8") as file:
        data = json.load(file)

    questions: List[Dict[str, Any]] = []
    for section, question_list in data.items():
        for question in question_list:
            questions.append({"section": section, "question": question})
    return questions


def load_completed_questions(log_path: str) -> Dict[str, Dict[str, Any]]:
    """Read JSONL log file and return aggregated question results.

    Returns a mapping:
        {
            "<question>": {
                "section": "<section_name>",
                "passed": bool,       # True if any attempt had judge_label == "YES"
                "attempts": int       # Number of attempts logged
            },
            ...
        }

    Questions with passed == True will be skipped in the main loop.
    """
    results: Dict[str, Dict[str, Any]] = {}

    try:
        with open(log_path, "r", encoding="utf-8") as file:
            for line in file:
                line = line.strip()
                if not line:
                    continue

                entry = json.loads(line)
                question = entry["question"]

                # Initialize if not present
                if question not in results:
                    results[question] = {
                        "section": entry["section"],
                        "passed": False,
                        "attempts": 0,
                    }

                results[question]["attempts"] += 1

                # If any attempt for this question has YES, consider it passed
                if entry.get("judge_label") == "YES":
                    results[question]["passed"] = True

    except FileNotFoundError:
        print(
            f"[INFO] Log file '{log_path}' does not exist yet. "
            "Assuming no question has passed."
        )

    return results


def append_log(entry: Dict[str, Any]) -> None:
    """Append a single JSON entry as one line in the JSONL log file."""
    with open(LOG_FILE, "a", encoding="utf-8") as file:
        file.write(json.dumps(entry, ensure_ascii=False) + "\n")


def judge_answer(question: str, answer: str) -> Tuple[bool, str, str]:
    """Use an LLM judge to rate the answer.

    Args:
        question: Original user question.
        answer: Answer generated by the RAG system.

    Returns:
        is_ok: True if judged as YES, otherwise False.
        reason: Parsed reason from the judge response.
        raw: Raw judge response string.
    """
    judge_prompt = f"""
You evaluate whether an answer meaningfully and correctly addresses the user question.

Question:
{question}

Answer:
{answer}

Your response must be:
YES | <reason>
or
NO | <reason>

Do not add any other text, no bullet points, no quotes.
"""

    response = client.models.generate_content(
        model=JUDGE_MODEL,
        contents=judge_prompt,
    )

    raw = response.text.strip()

    if "|" in raw:
        tag, reason = raw.split("|", 1)
        tag = tag.strip().upper()
        reason = reason.strip()
    else:
        tag = "NO"
        reason = f"Unexpected judge format: {raw}"

    is_ok = tag == "YES"
    return is_ok, reason, raw


def main() -> None:
    """Main evaluation loop."""
    questions = load_questions(QUESTIONS_FILE)
    completed = load_completed_questions(LOG_FILE)

    print(f"Loaded {len(questions)} questions.")
    print(f"Found {len(completed)} logged questions.")

    for index, item in enumerate(questions, start=1):
        question = item["question"]
        section = item["section"]

        # Skip if question already passed
        if question in completed and completed[question]["passed"]:
            print(f"\n>>> SKIP: Question already passed: {question}")
            continue

        print("\n" + "=" * 80)
        print(f"QUESTION {index} ({section}): {question}")

        success = False

        for attempt in range(1, ATTEMPTS_PER_QUESTION + 1):
            print(f"\n--- Attempt {attempt}/{ATTEMPTS_PER_QUESTION} ---")
            print("Sleeping 30 seconds before calling RAG...")
            time.sleep(30)

            rag_result = generate_answer(question)
            answer_text = rag_result["answer"]
            sources = rag_result.get("sources", [])

            print("RAG Answer:\n", answer_text)

            print("Sleeping 30 seconds before calling Judge...")
            time.sleep(30)

            is_ok, judge_reason, judge_raw = judge_answer(question, answer_text)
            label = "YES" if is_ok else "NO"

            print("Judge:", label, "-", judge_reason)

            log_entry: Dict[str, Any] = {
                "section": section,
                "question": question,
                "attempt": attempt,
                "answer": answer_text,
                "sources": sources,
                "judge_label": label,
                "judge_reason": judge_reason,
                "judge_raw": judge_raw,
            }
            append_log(log_entry)

            if is_ok:
                print(">>> This question PASSED.")
                success = True
                break

        if not success:
            print(">>> This question FAILED after all attempts.")


if __name__ == "__main__":
    main()
